{
  "hash": "7a66420a11097bbe1e6bac069cc73f72",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Specifies the LLM provider and model to use during the R session\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n## lang_use\n\n## Description\nAllows us to specify the back-end provider, model to use during the current R session. The target language is not processed by the function, as in converting \"english\" to \"en\" for example. The value is passed directly to the LLM, and it lets the LLM interpret the target language.\n\n\n## Usage\n```r\nlang_use(\n  backend = NULL,\n  model = NULL,\n  .cache = NULL,\n  .lang = NULL,\n  .silent = FALSE,\n  ...\n)\n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| backend | \"ollama\" or an `ellmer` `Chat` object. If using \"ollama\", `mall` will use is out-of-the-box integration with that back-end. Defaults to \"ollama\". |\n| model | The name of model supported by the back-end provider |\n| .cache | The path to save model results, so they can be re-used if the same operation is ran again. To turn off, set this argument to an empty character: `\"\"`. It defaults to a temp folder. If this argument is left `NULL` when calling this function, no changes to the path will be made. |\n| .lang | Target language to translate to. This will override values found in the LANG and LANGUAGE environment variables. |\n| .silent | Boolean flag that controls if there is or not output to the console. Defaults to FALSE. |\n| ... | Additional arguments that this function will pass down to the integrating function. In the case of Ollama, it will pass those arguments to `ollamar::chat()`. |\n\n\n## Value\nConsole output of the current LLM setup to be used during the R session.\n\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n\n\nlibrary(lang)\n\n# Using an `ellmer` chat object\nlang_use(ellmer::chat_openai(model = \"gpt-4o\"))\n#> Model: gpt-4o via OpenAI\n#> Lang: en_US.UTF-8\n\n# Using Ollama directly\nlang_use(\"ollama\", \"llama3.2\", seed = 100)\n#> Model: llama3.2 via Ollama\n#> Lang: en_US.UTF-8\n\n# Turn off cache by setting `.cache` to \"\"\nlang_use(\"ollama\", \"llama3.2\", seed = 100, .cache = \"\")\n#> Model: llama3.2 via Ollama\n#> Lang: en_US.UTF-8\n#> Cache: [Disabled]\n\n# Use `.lang` to set the target language to translate to,\n# it will be set for the current R session\nlang_use(\"ollama\", \"llama3.2\", .lang = \"spanish\")\n#> Model: llama3.2 via Ollama\n#> Lang: spanish\n#> Cache: [Disabled]\n\n# Use `.silent` to avoid console output\nlang_use(\"ollama\", \"llama3.2\", .lang = \"spanish\", .silent = TRUE)\n\n# To see current settings, simply call the function\nlang_use()\n#> Model: llama3.2 via Ollama\n#> Lang: spanish\n#> Cache: [Disabled]\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}